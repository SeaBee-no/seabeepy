{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c44bc7-58dc-49ca-a0a3-05fcbfffeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "from config import SETTINGS\n",
    "\n",
    "import seabeepy as sb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b06922f5-55df-47ba-9326-d9e5319d9a35",
   "metadata": {},
   "source": [
    "# Upload NIVA datasets\n",
    "\n",
    "Files within the `niva` bucket are arranged in a fairly complex hierarchy. This can probably be simplified, but it's going to take a while.\n",
    "\n",
    "This notebook searches a \"mission\" folder for orthomosaics with user-specified names. For example, Hege sent me a list of mosaic names for the Kelpmap project (which I hadn't previously found because they're quite deeply buried). The orginal file names are generally not very helpful, so in this notebook the user must specify more suitable names and metadata for each file. This is done manually. The code then performs the following operations:\n",
    "\n",
    " 1. Finds the files on MinIO\n",
    " 2. Builds 3-band COGs for all datasets\n",
    " 3. Uploads the files to GeoServer\n",
    " 4. Publishes them to GeoNode\n",
    " 5. Updates the basic metadata by extracting information from the user-specified file names\n",
    " \n",
    "## 1. User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95780960-5933-4fb0-b8bf-a4687d3a0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mission folder to search\n",
    "base_dir = (\n",
    "    r\"/home/notebook/shared-seabee-ns9879k/niva/2022/2022-08-31_RUNDE/2022-08-31_Remoy/\"\n",
    ")\n",
    "\n",
    "# Input files to search for, mapped to more helpful output file names:\n",
    "#     region_area_org_spec_date-time.tif\n",
    "fnames_dict = {\n",
    "    \"rgb_total_composite.tif\": \"Runde_Remoy_NTNU_HSI_20220831-0800.tif\",\n",
    "    # \"20220819_1230_RGB_120m_transparent_mosaic_group1.tif\": \"KELPMAP_Vega-North_Spectrofly_RGB_20220819-1230.tif\",\n",
    "    # \"KelpMap_S_20220818_MS_v2.tif\": \"KELPMAP_Vega-South_Spectrofly_MS_20220818-0000.tif\",\n",
    "    # \"1055rgb120_transparent_mosaic_group1.tif\": \"KELPMAP_Vega-North_NIVA_RGB_20220818-1055.tif\",\n",
    "    # \"0944_ms_120_composite.tif\": \"KELPMAP_Vega-North_NIVA_MS_20220819-0944.tif\",\n",
    "    # \"0814_rgb_115_transparent_mosaic_group1.tif\": \"KELPMAP_Vega-South_NIVA_RGB_20220819-0814.tif\",\n",
    "    # \"0814_ms_115_composite.tif\": \"KELPMAP_Vega-South_NIVA_MS_20220819-0814.tif\",\n",
    "    # \"1012_rgb_60_transparent_mosaic_group1.tif\": \"KELPMAP_Vega-South_NIVA_RGB_20220819-1012.tif\",\n",
    "}\n",
    "\n",
    "# Temp folder with 'write' access\n",
    "cog_fold = r\"/home/notebook/cogs/\"\n",
    "\n",
    "# Raster properties\n",
    "no_data = 0\n",
    "red_band = 1\n",
    "green_band = 2\n",
    "blue_band = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6dd9c9e-b13f-4e1a-b7f3-859bfc4cda9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output names are unique\n",
    "assert len(set(fnames_dict.values())) == len(fnames_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06717cf-28d4-444b-a19d-d3b0b64a29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_by_name(fname, parent_folder):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        if fname in files:\n",
    "            result.append(os.path.join(root, fname))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcab4d87-6ae1-4f4f-a04e-de1abbcd62d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/notebook/shared-seabee-ns9879k/niva/2022/2022-08-31_RUNDE/2022-08-31_Remoy/1_drone/1_HSI/2022-08-31_0800_HSI/results/rgb_total_composite.tif']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist = []\n",
    "for fname in fnames_dict.keys():\n",
    "    fpaths = find_file_by_name(fname, base_dir)\n",
    "    if len(fpaths) > 1:\n",
    "        print(fpaths)\n",
    "    elif len(fpaths) == 0:\n",
    "        print(\"Could not find:\", fname)\n",
    "    else:\n",
    "        flist.append(fpaths[0])\n",
    "flist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3786e915-acae-457c-8fe6-db92e19f57a8",
   "metadata": {},
   "source": [
    "## 2. Process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7871f128-284c-497f-8982-ba4380872aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to MinIO\n",
    "minio_client = sb.storage.minio_login(\n",
    "    user=SETTINGS.MINIO_ACCESS_ID, password=SETTINGS.MINIO_SECRET_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42a39a6-0d1a-4672-af86-b07068208e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Runde_Remoy_NTNU_HSI_20220831-0800.tif\n",
      "  Converting to COG...\n",
      "Input file size is 6953, 5319\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "  Copying COG to MinIO...\n",
      "  Uploading to GeoServer...\n",
      "  Publishing to GeoNode...\n",
      "  Updating metadata...\n",
      "  Done.\n"
     ]
    }
   ],
   "source": [
    "for fpath in flist:\n",
    "    mission_fold, fname = os.path.split(fpath)\n",
    "    cog_path = os.path.join(cog_fold, fnames_dict[fname])\n",
    "    minio_path = os.path.join(mission_fold, fnames_dict[fname])\n",
    "\n",
    "    print(\"Processing:\", fnames_dict[fname])\n",
    "    print(\"  Converting to COG...\")\n",
    "\n",
    "    sb.geo.standardise_orthophoto(\n",
    "        fpath,\n",
    "        cog_path,\n",
    "        red_band=red_band,\n",
    "        green_band=green_band,\n",
    "        blue_band=blue_band,\n",
    "        nodata=no_data,\n",
    "    )\n",
    "\n",
    "    print(\"  Copying COG to MinIO...\")\n",
    "\n",
    "    sb.storage.copy_file(cog_path, minio_path, minio_client, overwrite=False)\n",
    "    os.remove(cog_path)\n",
    "\n",
    "    print(\"  Uploading to GeoServer...\")\n",
    "\n",
    "    layer_name = sb.geo.upload_raster_to_geoserver(\n",
    "        minio_path,\n",
    "        SETTINGS.GEOSERVER_USER,\n",
    "        SETTINGS.GEOSERVER_PASSWORD,\n",
    "        workspace=\"geonode\",\n",
    "    )\n",
    "\n",
    "    print(\"  Publishing to GeoNode...\")\n",
    "\n",
    "    sb.geo.publish_to_geonode(\n",
    "        layer_name,\n",
    "        SETTINGS.GEONODE_USER,\n",
    "        SETTINGS.GEONODE_PASSWORD,\n",
    "        workspace=\"geonode\",\n",
    "    )\n",
    "\n",
    "    print(\"  Updating metadata...\")\n",
    "\n",
    "    # Extract metadata from names with format:\n",
    "    #    region_area_org_spec_date-time.tif\n",
    "    region, area, org, spec, date = layer_name.split(\"_\")\n",
    "    date = dt.datetime.strptime(date, \"%Y%m%d-%H%M\")\n",
    "    abstract = (\n",
    "        f\"{spec} mosaic collected by {org} for the {region} survey at {area} on {date}.\"\n",
    "        f\"<br><br><b>MinIO file name:</b> {minio_path}.\"\n",
    "    )\n",
    "\n",
    "    # Update metadata\n",
    "    metadata = {\n",
    "        \"abstract\": abstract,\n",
    "        \"date\": date.isoformat(),\n",
    "        \"date_type\": \"creation\",\n",
    "        \"attribution\": \"SeaBee\",\n",
    "    }\n",
    "    sb.geo.update_geonode_metadata(\n",
    "        layer_name,\n",
    "        SETTINGS.GEONODE_USER,\n",
    "        SETTINGS.GEONODE_PASSWORD,\n",
    "        metadata,\n",
    "    )\n",
    "\n",
    "    print(\"  Done.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "412ab0ef",
   "metadata": {},
   "source": [
    "## Update keywords and ISO fields\n",
    "\n",
    "Geonode have a rest API for datasets on `/api/v2/datasets`, but updating `keywords` and `tkeywords` does not seem to work. One (temporary) approach to achieve it is to:\n",
    "\n",
    "1. Get the dataset using the geonode api\n",
    "2. Get the full iso `MD_Metadata` using the csw endpoint `/catalogue/csw`\n",
    "    - note pycsw does not support transactions for geonode\n",
    "3. Modify the iso record with seabeepy's gmd package or lxml would also work\n",
    "4. Login with a csrf token and upload the iso file on `/datasets/upload`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4eb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install git+https://github.com/SeaBee-no/seabeepy.git\n",
    "# ignore owslib future warning\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import requests\n",
    "\n",
    "# used to marshall the metadata xml\n",
    "from xsdata.formats.dataclass import serializers\n",
    "\n",
    "from seabeepy.metadata import gmd, utils\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56820685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_iso_metadata(\n",
    "    ds_metadata: List[Tuple[str, gmd.MdMetadata]], geonode_url\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"Upload iso for a list of datasets\n",
    "\n",
    "    Login to geonode and get a csrf token so we are allowed to post to `dataset/upload`\n",
    "    \"\"\"\n",
    "    serializer = serializers.XmlSerializer()\n",
    "    response_list = []\n",
    "    login_url = f\"{geonode_url}/account/login/\"\n",
    "\n",
    "    def post_metadata(client, title: str, ds_meta: gmd.MdMetadata, csrftoken: str):\n",
    "        return client.post(\n",
    "            f\"{geonode_url}/datasets/upload\",\n",
    "            files={\n",
    "                \"base_file\": (\n",
    "                    \"sample.xml\",\n",
    "                    serializer.render(ds_meta),\n",
    "                    \"text/xml\",\n",
    "                ),\n",
    "            },\n",
    "            data={\n",
    "                \"permissions\": \"{}\",\n",
    "                \"charset\": \"undefined\",\n",
    "                \"metadata_upload_form\": \"true\",\n",
    "                \"dataset_title\": title,\n",
    "            },\n",
    "            headers={\n",
    "                # https://geonode.seabee.sigma2.no/datasets/geonode:test_workshop_Kunstcafe_transparent_mosaic_group1/metadata_upload\n",
    "                \"Referer\": login_url,\n",
    "                \"X-CSRFToken\": csrftoken,\n",
    "                \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "            },\n",
    "            cookies={\"csrftoken\": csrftoken},\n",
    "        )\n",
    "\n",
    "    client = requests.session()\n",
    "    client.get(login_url)\n",
    "    # Django would like the csrf token passed with the data, so we do need to save it off seperately.\n",
    "    csrftoken = client.cookies[\"csrftoken\"]\n",
    "    r = client.post(\n",
    "        login_url,\n",
    "        headers=dict(Referer=login_url),\n",
    "        data={\n",
    "            \"login\": SETTINGS.GEONODE_USER,\n",
    "            \"password\": SETTINGS.GEONODE_PASSWORD,\n",
    "            \"csrfmiddlewaretoken\": csrftoken,\n",
    "        },\n",
    "    )\n",
    "    # For some reason, we are issued a new csrf token after logging in, so update your local copy.\n",
    "    csrftoken = client.cookies[\"csrftoken\"]\n",
    "\n",
    "    for title, ds_meta in ds_metadata:\n",
    "        resp = post_metadata(client, title, ds_meta, csrftoken)\n",
    "        response_list.append(resp)\n",
    "    return response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(url) -> List[Dict]:\n",
    "    resp = requests.get(url).json()\n",
    "    datasets = resp[\"datasets\"]\n",
    "    if resp[\"links\"][\"next\"] is not None:\n",
    "        datasets.extend(get_datasets(resp[\"links\"][\"next\"]))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919269a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using some older and unregular routes:)\n",
    "# So not part of v2 api\n",
    "geonode_url = \"https://geonode.seabee.sigma2.no\"\n",
    "# setup xml parsers\n",
    "serializer = serializers.XmlSerializer(\n",
    "    config=serializers.config.SerializerConfig(pretty_print=True)\n",
    ")\n",
    "\n",
    "resp = requests.get(f\"{geonode_url}/api/v2/datasets\").json()\n",
    "\n",
    "datasets = get_datasets(f\"{geonode_url}/api/v2/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0927535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ds for ds in datasets if ds[\"subtype\"] == \"raster\"]\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45839276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the list of metadata instance\n",
    "# this also allow us to marshall all of them to disk\n",
    "# We add the title(`alternate`) from the ds along with iso record\n",
    "ds_metadata_list = []\n",
    "\n",
    "for ds in datasets:\n",
    "    ds_meta = utils.fetch_dataset_iso(ds[\"uuid\"], geonode_url)\n",
    "    abstract = ds_meta.identification_info[\n",
    "        0\n",
    "    ].md_data_identification.abstract.character_string\n",
    "    print(\n",
    "        ds_meta.identification_info[\n",
    "            0\n",
    "        ].md_data_identification.citation.ci_citation.title.character_string\n",
    "    )\n",
    "    print(abstract)\n",
    "    ds_meta = utils.remove_all_keywords(ds_meta)\n",
    "    # Could also just remove norwegian keywords if keeping custom keywords\n",
    "    # ds_meta = utils.remove_norwegian_thesarus(ds_meta)\n",
    "    # Add norwegian keywords\n",
    "    # See https://register.geonorge.no/metadata-kodelister/inspiretema\n",
    "    ds_meta = utils.add_norwegian_thesarus_keywords(\n",
    "        ds_meta, [\"Ortofoto\", \"Habitater og biotoper\"]\n",
    "    )\n",
    "    # Add custom seabee keywords\n",
    "    keywords = [\"SeaBee\"]\n",
    "    if \"NIVA\" in abstract:\n",
    "        keywords.append(\"NIVA\")\n",
    "    if \"NINA\" in abstract:\n",
    "        keywords.append(\"NINA\")\n",
    "    if \"Spectrofly\" in abstract:\n",
    "        keywords.append(\"Spectrofly\")\n",
    "    if \"KELPMAP\" in abstract:\n",
    "        keywords.append(\"KELPMAP\")\n",
    "\n",
    "    ds_meta = utils.add_seabee_keywords(ds_meta, keywords)\n",
    "    ds_metadata_list.append((ds[\"alternate\"], ds_meta))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7695e58",
   "metadata": {},
   "source": [
    "### We can marshall the python object to get the xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cbb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/sample.xml\", \"w\") as f:\n",
    "    f.write(serializer.render(ds_metadata_list[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf49d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_list = upload_iso_metadata(ds_metadata_list, geonode_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in resp_list:\n",
    "    res = r.json()\n",
    "    print(res)\n",
    "    print(res[\"status\"])\n",
    "    print(f\"{geonode_url}{res['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbaf3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resp_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "69d0b74f1e5111a26363d8ed5035edb8ad5befc32b6ba89d070f1858b7178fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
